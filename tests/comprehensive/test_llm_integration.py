#!/usr/bin/env python3
"""
Comprehensive LLM Integration Test
Tests if Ollama and the LLM model are working properly with the backend.
"""

import requests
import time
import sys
import os

# Add app to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))

def test_ollama_server():
    """Test if Ollama server is running"""
    print("üîç Testing Ollama Server Connection...")
    
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            models = data.get("models", [])
            print(f"   ‚úÖ Ollama server is running")
            print(f"   üìã Available models: {len(models)}")
            
            # Check if our target model is available
            target_model = "hf.co/defog/sqlcoder-7b-2:latest"
            model_names = [model.get("name", "") for model in models]
            
            if target_model in model_names:
                print(f"   ‚úÖ Target model '{target_model}' is available")
                return True, target_model
            else:
                print(f"   ‚ö†Ô∏è Target model '{target_model}' not found")
                print(f"   üìù Available models: {model_names}")
                return False, target_model
        else:
            print(f"   ‚ùå Ollama server returned status {response.status_code}")
            return False, None
            
    except requests.exceptions.ConnectionError:
        print("   ‚ùå Cannot connect to Ollama server at localhost:11434")
        print("   üí° Start with: ollama serve")
        return False, None
    except Exception as e:
        print(f"   ‚ùå Error checking Ollama: {e}")
        return False, None

def test_ollama_model_direct(model_name):
    """Test the model directly via Ollama API"""
    print(f"\nü§ñ Testing Model '{model_name}' Directly...")
    
    try:
        payload = {
            "model": model_name,
            "prompt": "Generate only this SQL: SELECT 1 as test",
            "stream": False,
            "options": {
                "num_ctx": 1024,
                "temperature": 0
            }
        }
        
        response = requests.post(
            "http://localhost:11434/api/generate", 
            json=payload, 
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            response_text = data.get("response", "").strip()
            print(f"   ‚úÖ Model responded successfully")
            print(f"   üìù Response: {response_text[:100]}...")
            print(f"   ‚è±Ô∏è Done: {data.get('done', False)}")
            return True, response_text
        else:
            print(f"   ‚ùå Model request failed with status {response.status_code}")
            print(f"   üìÑ Response: {response.text[:200]}...")
            return False, None
            
    except Exception as e:
        print(f"   ‚ùå Error testing model: {e}")
        return False, None

def test_backend_health():
    """Test if the backend is running and healthy"""
    print("\nüè• Testing Backend Health...")
    
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        if response.status_code == 200:
            data = response.json()
            print(f"   ‚úÖ Backend is running")
            print(f"   üìä Status: {data.get('status', 'unknown')}")
            
            services = data.get("services", {})
            for service, status in services.items():
                emoji = "‚úÖ" if status == "healthy" else "‚ö†Ô∏è" if status == "degraded" else "‚ùå"
                print(f"   {emoji} {service}: {status}")
            
            return True, data
        else:
            print(f"   ‚ùå Backend health check failed: {response.status_code}")
            return False, None
            
    except requests.exceptions.ConnectionError:
        print("   ‚ùå Cannot connect to backend at localhost:8000")
        print("   üí° Start with: uv run python run_app.py")
        return False, None
    except Exception as e:
        print(f"   ‚ùå Error checking backend health: {e}")
        return False, None

def test_llm_via_backend():
    """Test LLM integration through the backend API"""
    print("\nüîó Testing LLM Integration via Backend API...")
    
    test_questions = [
        "SELECT 1 as one",
        "Show all customers",
        "Find the total number of accounts"
    ]
    
    results = []
    
    for i, question in enumerate(test_questions, 1):
        print(f"   üß™ Test {i}: '{question}'")
        
        try:
            payload = {
                "question": question,
                "role": "analyst",
                "user": "llm_test_user"
            }
            
            response = requests.post(
                "http://localhost:8000/ask",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                success = data.get("success", False)
                sql = data.get("explanation", "")
                table = data.get("table", {})
                error = table.get("error", "")
                
                if success and sql and not error:
                    print(f"      ‚úÖ Success: {sql[:60]}...")
                    results.append(True)
                else:
                    print(f"      ‚ùå Failed: {error or 'No SQL generated'}")
                    results.append(False)
                    
            else:
                print(f"      ‚ùå API error: {response.status_code}")
                results.append(False)
                
        except Exception as e:
            print(f"      ‚ùå Request error: {e}")
            results.append(False)
        
        time.sleep(1)  # Brief pause between requests
    
    success_rate = sum(results) / len(results) if results else 0
    print(f"\n   üìä Success rate: {len([r for r in results if r])}/{len(results)} ({success_rate:.1%})")
    
    return success_rate > 0.5, results

def test_cache_clearing():
    """Test cache clearing functionality"""
    print("\nüóëÔ∏è Testing Cache Clearing...")
    
    try:
        response = requests.post("http://localhost:8000/admin/clear-cache", timeout=10)
        if response.status_code == 200:
            data = response.json()
            print(f"   ‚úÖ Cache cleared successfully")
            print(f"   üìù Message: {data.get('message', 'No message')}")
            return True
        else:
            print(f"   ‚ùå Cache clear failed: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"   ‚ùå Error clearing cache: {e}")
        return False

def main():
    """Run comprehensive LLM integration test"""
    print("üéØ LLM INTEGRATION TEST SUITE")
    print("=" * 60)
    
    results = []
    
    # Test 1: Ollama Server
    print("\nüìã Test 1: Ollama Server")
    ollama_ok, model_name = test_ollama_server()
    results.append(ollama_ok)
    
    # Test 2: Model Direct (if Ollama is working)
    if ollama_ok and model_name:
        print("\nüìã Test 2: Model Direct Test")
        model_ok, _ = test_ollama_model_direct(model_name)
        results.append(model_ok)
    else:
        print("\nüìã Test 2: Model Direct Test - SKIPPED (Ollama not available)")
        results.append(False)
    
    # Test 3: Backend Health
    print("\nüìã Test 3: Backend Health")
    backend_ok, _ = test_backend_health()
    results.append(backend_ok)
    
    # Test 4: LLM via Backend (if backend is working)
    if backend_ok:
        print("\nüìã Test 4: LLM Integration via Backend")
        llm_backend_ok, _ = test_llm_via_backend()
        results.append(llm_backend_ok)
    else:
        print("\nüìã Test 4: LLM Integration via Backend - SKIPPED (Backend not available)")
        results.append(False)
    
    # Test 5: Cache Clearing (if backend is working)
    if backend_ok:
        print("\nüìã Test 5: Cache Clearing")
        cache_ok = test_cache_clearing()
        results.append(cache_ok)
    else:
        print("\nüìã Test 5: Cache Clearing - SKIPPED (Backend not available)")
        results.append(False)
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä INTEGRATION TEST SUMMARY")
    print("=" * 60)
    
    passed = sum(results)
    total = len(results)
    
    test_names = [
        "Ollama Server",
        "Model Direct",
        "Backend Health", 
        "LLM via Backend",
        "Cache Clearing"
    ]
    
    for i, (name, result) in enumerate(zip(test_names, results)):
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} {name}")
    
    print(f"\nOverall: {passed}/{total} tests passed ({passed/total:.1%})")
    
    if passed == total:
        print("üéâ ALL TESTS PASSED! LLM integration is working perfectly.")
    elif passed >= 3:
        print("‚ö†Ô∏è MOST TESTS PASSED. LLM integration is mostly working.")
    else:
        print("‚ùå MANY TESTS FAILED. LLM integration needs attention.")
    
    print("\nüí° Next Steps:")
    if not results[0]:  # Ollama server
        print("- Start Ollama: ollama serve")
        print("- Pull model: ollama pull sqlcoder7b:latest")
    if not results[2]:  # Backend
        print("- Start backend: uv run python run_app.py")
    if results[2] and not results[3]:  # Backend ok but LLM integration fails
        print("- Check LLM logs in backend console")
        print("- Verify model is loaded in Ollama")
        print("- Clear cache: curl -X POST http://localhost:8000/admin/clear-cache")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
